#Configure Ollama local model
quarkus.langchain4j.ollama.chat-model.model-id=qwen2.5:0.5b

quarkus.langchain4j.ollama.chat-model.temperature=0.0
quarkus.langchain4j.ollama.log-requests=true
quarkus.langchain4j.log-responses=true
quarkus.langchain4j.ollama.timeout=180s

#Deploy to Developer Sandbox
%prod.quarkus.langchain4j.ollama.base-url=http://llm:8000/
%prod.quarkus.openshift.route.expose=true
%prod.quarkus.openshift.deploy=true

# routes to load
camel.main.routes-include-pattern = routes/*.yaml
